{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9032a2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 Training LSTM...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 0.0029 - mae: 0.0362 - val_loss: 7.2391e-04 - val_mae: 0.0194\n",
      "Epoch 2/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 8.8906e-04 - mae: 0.0213 - val_loss: 6.9388e-04 - val_mae: 0.0188\n",
      "Epoch 3/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 8.2441e-04 - mae: 0.0203 - val_loss: 6.3184e-04 - val_mae: 0.0174\n",
      "Epoch 4/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 7.4519e-04 - mae: 0.0186 - val_loss: 5.8731e-04 - val_mae: 0.0165\n",
      "Epoch 5/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 7ms/step - loss: 6.4576e-04 - mae: 0.0172 - val_loss: 5.6157e-04 - val_mae: 0.0161\n",
      "Epoch 6/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - loss: 6.3512e-04 - mae: 0.0168 - val_loss: 5.4665e-04 - val_mae: 0.0157\n",
      "Epoch 7/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - loss: 6.0160e-04 - mae: 0.0163 - val_loss: 5.4519e-04 - val_mae: 0.0154\n",
      "Epoch 8/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - loss: 5.7794e-04 - mae: 0.0157 - val_loss: 5.8678e-04 - val_mae: 0.0162\n",
      "Epoch 9/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 5.6942e-04 - mae: 0.0158 - val_loss: 5.2935e-04 - val_mae: 0.0149\n",
      "Epoch 10/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - loss: 5.3577e-04 - mae: 0.0153 - val_loss: 5.3468e-04 - val_mae: 0.0148\n",
      "Epoch 11/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - loss: 5.8105e-04 - mae: 0.0153 - val_loss: 5.2096e-04 - val_mae: 0.0147\n",
      "Epoch 12/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - loss: 5.3156e-04 - mae: 0.0149 - val_loss: 5.0777e-04 - val_mae: 0.0142\n",
      "Epoch 13/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 5.4020e-04 - mae: 0.0151 - val_loss: 5.0258e-04 - val_mae: 0.0142\n",
      "Epoch 14/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - loss: 5.5437e-04 - mae: 0.0149 - val_loss: 5.0760e-04 - val_mae: 0.0144\n",
      "Epoch 15/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 5.2466e-04 - mae: 0.0147 - val_loss: 4.9711e-04 - val_mae: 0.0139\n",
      "Epoch 16/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - loss: 5.3120e-04 - mae: 0.0148 - val_loss: 5.5069e-04 - val_mae: 0.0164\n",
      "Epoch 17/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 5.4095e-04 - mae: 0.0150 - val_loss: 4.9334e-04 - val_mae: 0.0137\n",
      "Epoch 18/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 5.2240e-04 - mae: 0.0146 - val_loss: 4.8989e-04 - val_mae: 0.0138\n",
      "Epoch 19/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 5.1548e-04 - mae: 0.0145 - val_loss: 5.3187e-04 - val_mae: 0.0156\n",
      "Epoch 20/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - loss: 5.5624e-04 - mae: 0.0148 - val_loss: 4.8595e-04 - val_mae: 0.0138\n",
      "Epoch 21/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - loss: 5.2000e-04 - mae: 0.0145 - val_loss: 5.1888e-04 - val_mae: 0.0143\n",
      "Epoch 22/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - loss: 4.8962e-04 - mae: 0.0144 - val_loss: 4.8843e-04 - val_mae: 0.0138\n",
      "Epoch 23/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - loss: 4.9963e-04 - mae: 0.0144 - val_loss: 5.1501e-04 - val_mae: 0.0145\n",
      "Epoch 24/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - loss: 5.0681e-04 - mae: 0.0143 - val_loss: 4.8775e-04 - val_mae: 0.0141\n",
      "Epoch 25/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - loss: 5.0849e-04 - mae: 0.0143 - val_loss: 4.9232e-04 - val_mae: 0.0141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model: model/lstm_model.h5\n",
      "\n",
      "🔧 Training GRU...\n",
      "Epoch 1/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - loss: 0.0021 - mae: 0.0317 - val_loss: 7.6248e-04 - val_mae: 0.0205\n",
      "Epoch 2/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 8.2586e-04 - mae: 0.0210 - val_loss: 7.2926e-04 - val_mae: 0.0197\n",
      "Epoch 3/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 11ms/step - loss: 7.4615e-04 - mae: 0.0197 - val_loss: 6.7906e-04 - val_mae: 0.0183\n",
      "Epoch 4/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 7.2485e-04 - mae: 0.0186 - val_loss: 6.4312e-04 - val_mae: 0.0166\n",
      "Epoch 5/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 6.6348e-04 - mae: 0.0179 - val_loss: 5.7205e-04 - val_mae: 0.0157\n",
      "Epoch 6/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - loss: 6.5696e-04 - mae: 0.0173 - val_loss: 5.7102e-04 - val_mae: 0.0159\n",
      "Epoch 7/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - loss: 6.2100e-04 - mae: 0.0166 - val_loss: 5.5698e-04 - val_mae: 0.0155\n",
      "Epoch 8/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 6.0604e-04 - mae: 0.0164 - val_loss: 5.7684e-04 - val_mae: 0.0161\n",
      "Epoch 9/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - loss: 6.0317e-04 - mae: 0.0161 - val_loss: 5.3262e-04 - val_mae: 0.0149\n",
      "Epoch 10/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 5.8113e-04 - mae: 0.0159 - val_loss: 5.4425e-04 - val_mae: 0.0152\n",
      "Epoch 11/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 11ms/step - loss: 5.8279e-04 - mae: 0.0159 - val_loss: 5.3013e-04 - val_mae: 0.0147\n",
      "Epoch 12/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 5.9426e-04 - mae: 0.0158 - val_loss: 5.2562e-04 - val_mae: 0.0145\n",
      "Epoch 13/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 5.4885e-04 - mae: 0.0154 - val_loss: 5.6453e-04 - val_mae: 0.0161\n",
      "Epoch 14/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - loss: 5.7377e-04 - mae: 0.0155 - val_loss: 5.3160e-04 - val_mae: 0.0152\n",
      "Epoch 15/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - loss: 5.2659e-04 - mae: 0.0152 - val_loss: 5.2940e-04 - val_mae: 0.0146\n",
      "Epoch 16/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - loss: 5.3416e-04 - mae: 0.0152 - val_loss: 5.8111e-04 - val_mae: 0.0167\n",
      "Epoch 17/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - loss: 5.6063e-04 - mae: 0.0153 - val_loss: 5.7233e-04 - val_mae: 0.0152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model: model/gru_model.h5\n",
      "\n",
      "🔧 Training BiLSTM...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 13ms/step - loss: 0.0024 - mae: 0.0339 - val_loss: 7.9477e-04 - val_mae: 0.0212\n",
      "Epoch 2/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - loss: 8.9330e-04 - mae: 0.0212 - val_loss: 6.5723e-04 - val_mae: 0.0185\n",
      "Epoch 3/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 12ms/step - loss: 7.7233e-04 - mae: 0.0197 - val_loss: 6.1558e-04 - val_mae: 0.0170\n",
      "Epoch 4/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 12ms/step - loss: 6.8329e-04 - mae: 0.0184 - val_loss: 6.0926e-04 - val_mae: 0.0172\n",
      "Epoch 5/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 12ms/step - loss: 6.5442e-04 - mae: 0.0177 - val_loss: 5.9094e-04 - val_mae: 0.0163\n",
      "Epoch 6/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - loss: 5.9214e-04 - mae: 0.0169 - val_loss: 5.5542e-04 - val_mae: 0.0155\n",
      "Epoch 7/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - loss: 6.1653e-04 - mae: 0.0165 - val_loss: 5.6117e-04 - val_mae: 0.0160\n",
      "Epoch 8/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 12ms/step - loss: 5.7040e-04 - mae: 0.0160 - val_loss: 5.5091e-04 - val_mae: 0.0157\n",
      "Epoch 9/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - loss: 5.8364e-04 - mae: 0.0159 - val_loss: 5.4806e-04 - val_mae: 0.0160\n",
      "Epoch 10/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 14ms/step - loss: 5.7747e-04 - mae: 0.0158 - val_loss: 5.3677e-04 - val_mae: 0.0148\n",
      "Epoch 11/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - loss: 5.3707e-04 - mae: 0.0153 - val_loss: 5.4998e-04 - val_mae: 0.0153\n",
      "Epoch 12/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 12ms/step - loss: 5.5275e-04 - mae: 0.0153 - val_loss: 5.4302e-04 - val_mae: 0.0152\n",
      "Epoch 13/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - loss: 5.5687e-04 - mae: 0.0155 - val_loss: 5.1591e-04 - val_mae: 0.0150\n",
      "Epoch 14/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - loss: 5.3894e-04 - mae: 0.0151 - val_loss: 5.0954e-04 - val_mae: 0.0143\n",
      "Epoch 15/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - loss: 5.3974e-04 - mae: 0.0150 - val_loss: 5.0122e-04 - val_mae: 0.0141\n",
      "Epoch 16/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - loss: 5.0928e-04 - mae: 0.0147 - val_loss: 5.0621e-04 - val_mae: 0.0145\n",
      "Epoch 17/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - loss: 5.0248e-04 - mae: 0.0146 - val_loss: 4.9609e-04 - val_mae: 0.0143\n",
      "Epoch 18/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - loss: 5.3272e-04 - mae: 0.0147 - val_loss: 4.9021e-04 - val_mae: 0.0140\n",
      "Epoch 19/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 12ms/step - loss: 5.2501e-04 - mae: 0.0147 - val_loss: 4.8776e-04 - val_mae: 0.0137\n",
      "Epoch 20/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - loss: 4.9114e-04 - mae: 0.0143 - val_loss: 4.9478e-04 - val_mae: 0.0141\n",
      "Epoch 21/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - loss: 5.2457e-04 - mae: 0.0146 - val_loss: 4.9102e-04 - val_mae: 0.0138\n",
      "Epoch 22/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - loss: 5.1552e-04 - mae: 0.0143 - val_loss: 4.9757e-04 - val_mae: 0.0140\n",
      "Epoch 23/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 18ms/step - loss: 5.1446e-04 - mae: 0.0145 - val_loss: 4.8421e-04 - val_mae: 0.0140\n",
      "Epoch 24/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - loss: 4.9236e-04 - mae: 0.0143 - val_loss: 4.8846e-04 - val_mae: 0.0137\n",
      "Epoch 25/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - loss: 5.1476e-04 - mae: 0.0144 - val_loss: 4.7785e-04 - val_mae: 0.0138\n",
      "Epoch 26/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - loss: 5.4217e-04 - mae: 0.0143 - val_loss: 4.7608e-04 - val_mae: 0.0136\n",
      "Epoch 27/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 13ms/step - loss: 5.1734e-04 - mae: 0.0143 - val_loss: 4.7505e-04 - val_mae: 0.0136\n",
      "Epoch 28/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 13ms/step - loss: 4.8673e-04 - mae: 0.0141 - val_loss: 4.9892e-04 - val_mae: 0.0147\n",
      "Epoch 29/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 12ms/step - loss: 5.1335e-04 - mae: 0.0142 - val_loss: 4.8817e-04 - val_mae: 0.0139\n",
      "Epoch 30/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 14ms/step - loss: 5.0182e-04 - mae: 0.0141 - val_loss: 4.8321e-04 - val_mae: 0.0136\n",
      "Epoch 31/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 14ms/step - loss: 5.0836e-04 - mae: 0.0142 - val_loss: 4.6612e-04 - val_mae: 0.0134\n",
      "Epoch 32/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 13ms/step - loss: 4.9623e-04 - mae: 0.0139 - val_loss: 4.8501e-04 - val_mae: 0.0135\n",
      "Epoch 33/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 13ms/step - loss: 5.0409e-04 - mae: 0.0140 - val_loss: 4.7612e-04 - val_mae: 0.0138\n",
      "Epoch 34/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 13ms/step - loss: 4.8236e-04 - mae: 0.0139 - val_loss: 4.6895e-04 - val_mae: 0.0136\n",
      "Epoch 35/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 13ms/step - loss: 4.9105e-04 - mae: 0.0139 - val_loss: 4.7759e-04 - val_mae: 0.0138\n",
      "Epoch 36/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 13ms/step - loss: 4.8396e-04 - mae: 0.0139 - val_loss: 4.6759e-04 - val_mae: 0.0132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model: model/bilstm_model.h5\n",
      "\n",
      "🔧 Training CNN...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.0014 - mae: 0.0244 - val_loss: 5.7624e-04 - val_mae: 0.0156\n",
      "Epoch 2/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 5.7415e-04 - mae: 0.0158 - val_loss: 5.4712e-04 - val_mae: 0.0144\n",
      "Epoch 3/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 6.0801e-04 - mae: 0.0158 - val_loss: 5.0309e-04 - val_mae: 0.0140\n",
      "Epoch 4/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 5.2076e-04 - mae: 0.0150 - val_loss: 4.9247e-04 - val_mae: 0.0134\n",
      "Epoch 5/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 5.3561e-04 - mae: 0.0147 - val_loss: 4.8544e-04 - val_mae: 0.0133\n",
      "Epoch 6/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 5.5342e-04 - mae: 0.0147 - val_loss: 4.8917e-04 - val_mae: 0.0140\n",
      "Epoch 7/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 5.2194e-04 - mae: 0.0145 - val_loss: 4.8464e-04 - val_mae: 0.0137\n",
      "Epoch 8/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 5.0093e-04 - mae: 0.0144 - val_loss: 5.7948e-04 - val_mae: 0.0158\n",
      "Epoch 9/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 4.9635e-04 - mae: 0.0146 - val_loss: 5.2120e-04 - val_mae: 0.0152\n",
      "Epoch 10/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 5.3687e-04 - mae: 0.0148 - val_loss: 4.9731e-04 - val_mae: 0.0143\n",
      "Epoch 11/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 5.0057e-04 - mae: 0.0143 - val_loss: 4.7551e-04 - val_mae: 0.0134\n",
      "Epoch 12/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 4.9416e-04 - mae: 0.0142 - val_loss: 4.7823e-04 - val_mae: 0.0135\n",
      "Epoch 13/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 4.9972e-04 - mae: 0.0142 - val_loss: 5.2866e-04 - val_mae: 0.0153\n",
      "Epoch 14/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 5.0093e-04 - mae: 0.0141 - val_loss: 5.5308e-04 - val_mae: 0.0149\n",
      "Epoch 15/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 4.8756e-04 - mae: 0.0140 - val_loss: 4.9855e-04 - val_mae: 0.0141\n",
      "Epoch 16/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 4.8869e-04 - mae: 0.0141 - val_loss: 4.7338e-04 - val_mae: 0.0132\n",
      "Epoch 17/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 4.9739e-04 - mae: 0.0141 - val_loss: 4.9488e-04 - val_mae: 0.0139\n",
      "Epoch 18/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 4.8989e-04 - mae: 0.0139 - val_loss: 4.9641e-04 - val_mae: 0.0142\n",
      "Epoch 19/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 4.7845e-04 - mae: 0.0138 - val_loss: 5.4716e-04 - val_mae: 0.0155\n",
      "Epoch 20/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 4.8964e-04 - mae: 0.0139 - val_loss: 4.7488e-04 - val_mae: 0.0129\n",
      "Epoch 21/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 5.0545e-04 - mae: 0.0139 - val_loss: 4.7375e-04 - val_mae: 0.0129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model: model/cnn_model.h5\n",
      "\n",
      "🔧 Training CNN-LSTM...\n",
      "Epoch 1/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - loss: 0.0027 - mae: 0.0330 - val_loss: 8.5394e-04 - val_mae: 0.0224\n",
      "Epoch 2/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 6.8429e-04 - mae: 0.0179 - val_loss: 6.4584e-04 - val_mae: 0.0179\n",
      "Epoch 3/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 6.8176e-04 - mae: 0.0174 - val_loss: 5.6956e-04 - val_mae: 0.0152\n",
      "Epoch 4/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 6.1227e-04 - mae: 0.0164 - val_loss: 5.3711e-04 - val_mae: 0.0149\n",
      "Epoch 5/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 6.0952e-04 - mae: 0.0162 - val_loss: 5.2330e-04 - val_mae: 0.0148\n",
      "Epoch 6/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 6.0063e-04 - mae: 0.0161 - val_loss: 5.5128e-04 - val_mae: 0.0152\n",
      "Epoch 7/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 5.9984e-04 - mae: 0.0161 - val_loss: 5.1707e-04 - val_mae: 0.0144\n",
      "Epoch 8/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 6.0974e-04 - mae: 0.0159 - val_loss: 5.6252e-04 - val_mae: 0.0154\n",
      "Epoch 9/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 5.9095e-04 - mae: 0.0156 - val_loss: 5.2008e-04 - val_mae: 0.0148\n",
      "Epoch 10/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 5.7760e-04 - mae: 0.0155 - val_loss: 5.3355e-04 - val_mae: 0.0145\n",
      "Epoch 11/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 5.5033e-04 - mae: 0.0152 - val_loss: 4.9122e-04 - val_mae: 0.0140\n",
      "Epoch 12/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 5.8236e-04 - mae: 0.0154 - val_loss: 5.3811e-04 - val_mae: 0.0155\n",
      "Epoch 13/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 5.5112e-04 - mae: 0.0153 - val_loss: 5.0682e-04 - val_mae: 0.0146\n",
      "Epoch 14/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 5.3336e-04 - mae: 0.0150 - val_loss: 4.9314e-04 - val_mae: 0.0141\n",
      "Epoch 15/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 5.5268e-04 - mae: 0.0151 - val_loss: 5.1189e-04 - val_mae: 0.0138\n",
      "Epoch 16/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 5.2222e-04 - mae: 0.0148 - val_loss: 4.9067e-04 - val_mae: 0.0142\n",
      "Epoch 17/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 5.2499e-04 - mae: 0.0149 - val_loss: 4.9110e-04 - val_mae: 0.0141\n",
      "Epoch 18/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 5.4038e-04 - mae: 0.0148 - val_loss: 4.9503e-04 - val_mae: 0.0142\n",
      "Epoch 19/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 5.1909e-04 - mae: 0.0147 - val_loss: 5.0722e-04 - val_mae: 0.0143\n",
      "Epoch 20/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 4.9578e-04 - mae: 0.0145 - val_loss: 5.4953e-04 - val_mae: 0.0165\n",
      "Epoch 21/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 5.4148e-04 - mae: 0.0149 - val_loss: 4.9179e-04 - val_mae: 0.0140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model: model/cnn_lstm_model.h5\n",
      "\n",
      "🔧 Training RNN...\n",
      "Epoch 1/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0051 - mae: 0.0381 - val_loss: 7.6060e-04 - val_mae: 0.0202\n",
      "Epoch 2/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 9.7389e-04 - mae: 0.0229 - val_loss: 0.0012 - val_mae: 0.0268\n",
      "Epoch 3/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.7928e-04 - mae: 0.0217 - val_loss: 7.2763e-04 - val_mae: 0.0200\n",
      "Epoch 4/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.1225e-04 - mae: 0.0207 - val_loss: 6.8458e-04 - val_mae: 0.0186\n",
      "Epoch 5/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.0250e-04 - mae: 0.0206 - val_loss: 6.9037e-04 - val_mae: 0.0192\n",
      "Epoch 6/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.5875e-04 - mae: 0.0196 - val_loss: 6.7474e-04 - val_mae: 0.0184\n",
      "Epoch 7/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 8.1622e-04 - mae: 0.0200 - val_loss: 7.0768e-04 - val_mae: 0.0191\n",
      "Epoch 8/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 7.4474e-04 - mae: 0.0189 - val_loss: 6.9910e-04 - val_mae: 0.0183\n",
      "Epoch 9/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.8823e-04 - mae: 0.0184 - val_loss: 6.1664e-04 - val_mae: 0.0169\n",
      "Epoch 10/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.5443e-04 - mae: 0.0178 - val_loss: 6.6744e-04 - val_mae: 0.0181\n",
      "Epoch 11/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.5265e-04 - mae: 0.0177 - val_loss: 5.8174e-04 - val_mae: 0.0161\n",
      "Epoch 12/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.4627e-04 - mae: 0.0172 - val_loss: 6.3550e-04 - val_mae: 0.0166\n",
      "Epoch 13/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2506e-04 - mae: 0.0171 - val_loss: 5.6466e-04 - val_mae: 0.0158\n",
      "Epoch 14/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 6.4598e-04 - mae: 0.0170 - val_loss: 6.3858e-04 - val_mae: 0.0180\n",
      "Epoch 15/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 6.0248e-04 - mae: 0.0166 - val_loss: 6.2004e-04 - val_mae: 0.0163\n",
      "Epoch 16/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.1939e-04 - mae: 0.0165 - val_loss: 5.5718e-04 - val_mae: 0.0159\n",
      "Epoch 17/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.8108e-04 - mae: 0.0163 - val_loss: 5.9384e-04 - val_mae: 0.0169\n",
      "Epoch 18/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.9198e-04 - mae: 0.0160 - val_loss: 5.4850e-04 - val_mae: 0.0154\n",
      "Epoch 19/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2156e-04 - mae: 0.0162 - val_loss: 5.7960e-04 - val_mae: 0.0164\n",
      "Epoch 20/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.8009e-04 - mae: 0.0158 - val_loss: 5.7793e-04 - val_mae: 0.0159\n",
      "Epoch 21/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 6.0369e-04 - mae: 0.0160 - val_loss: 5.9530e-04 - val_mae: 0.0171\n",
      "Epoch 22/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 6.2837e-04 - mae: 0.0161 - val_loss: 5.4740e-04 - val_mae: 0.0152\n",
      "Epoch 23/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 5.7714e-04 - mae: 0.0157 - val_loss: 5.5067e-04 - val_mae: 0.0152\n",
      "Epoch 24/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.8412e-04 - mae: 0.0157 - val_loss: 6.0461e-04 - val_mae: 0.0163\n",
      "Epoch 25/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.9899e-04 - mae: 0.0158 - val_loss: 5.5176e-04 - val_mae: 0.0155\n",
      "Epoch 26/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 5.8543e-04 - mae: 0.0158 - val_loss: 5.4241e-04 - val_mae: 0.0149\n",
      "Epoch 27/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.5655e-04 - mae: 0.0155 - val_loss: 7.0480e-04 - val_mae: 0.0198\n",
      "Epoch 28/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.7016e-04 - mae: 0.0155 - val_loss: 5.3400e-04 - val_mae: 0.0151\n",
      "Epoch 29/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.6505e-04 - mae: 0.0155 - val_loss: 5.4518e-04 - val_mae: 0.0155\n",
      "Epoch 30/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.8670e-04 - mae: 0.0155 - val_loss: 5.5224e-04 - val_mae: 0.0153\n",
      "Epoch 31/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.7825e-04 - mae: 0.0156 - val_loss: 5.3433e-04 - val_mae: 0.0145\n",
      "Epoch 32/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.8101e-04 - mae: 0.0153 - val_loss: 5.6146e-04 - val_mae: 0.0155\n",
      "Epoch 33/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.5163e-04 - mae: 0.0152 - val_loss: 5.2080e-04 - val_mae: 0.0145\n",
      "Epoch 34/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.4270e-04 - mae: 0.0152 - val_loss: 5.9590e-04 - val_mae: 0.0158\n",
      "Epoch 35/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.1714e-04 - mae: 0.0150 - val_loss: 5.6237e-04 - val_mae: 0.0157\n",
      "Epoch 36/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.2850e-04 - mae: 0.0150 - val_loss: 5.1842e-04 - val_mae: 0.0146\n",
      "Epoch 37/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.5352e-04 - mae: 0.0151 - val_loss: 5.1501e-04 - val_mae: 0.0143\n",
      "Epoch 38/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.7816e-04 - mae: 0.0153 - val_loss: 5.5022e-04 - val_mae: 0.0152\n",
      "Epoch 39/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.2144e-04 - mae: 0.0149 - val_loss: 5.1351e-04 - val_mae: 0.0141\n",
      "Epoch 40/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.4486e-04 - mae: 0.0150 - val_loss: 5.6367e-04 - val_mae: 0.0167\n",
      "Epoch 41/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.3367e-04 - mae: 0.0148 - val_loss: 5.0112e-04 - val_mae: 0.0144\n",
      "Epoch 42/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.4961e-04 - mae: 0.0149 - val_loss: 5.1750e-04 - val_mae: 0.0152\n",
      "Epoch 43/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.3131e-04 - mae: 0.0148 - val_loss: 5.4451e-04 - val_mae: 0.0153\n",
      "Epoch 44/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.4753e-04 - mae: 0.0151 - val_loss: 4.9787e-04 - val_mae: 0.0142\n",
      "Epoch 45/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.3269e-04 - mae: 0.0147 - val_loss: 5.0009e-04 - val_mae: 0.0142\n",
      "Epoch 46/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.4464e-04 - mae: 0.0149 - val_loss: 5.0532e-04 - val_mae: 0.0144\n",
      "Epoch 47/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.2605e-04 - mae: 0.0148 - val_loss: 5.0890e-04 - val_mae: 0.0146\n",
      "Epoch 48/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.2717e-04 - mae: 0.0147 - val_loss: 4.9853e-04 - val_mae: 0.0142\n",
      "Epoch 49/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.4246e-04 - mae: 0.0149 - val_loss: 4.9471e-04 - val_mae: 0.0141\n",
      "Epoch 50/50\n",
      "\u001b[1m1276/1276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 5.1928e-04 - mae: 0.0146 - val_loss: 4.8900e-04 - val_mae: 0.0136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model: model/rnn_model.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, LSTM, GRU, SimpleRNN, Conv1D, MaxPooling1D,\n",
    "    Flatten, Bidirectional\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import joblib\n",
    "\n",
    "# ------------------------ Configuration ------------------------\n",
    "selected_model = \"all\"  # Choose: \"LSTM\", \"GRU\", \"BiLSTM\", \"CNN\", \"CNN-LSTM\", \"RNN\", or \"all\"\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "SEQUENCE_LENGTH = 24\n",
    "\n",
    "# ------------------------ Load and Preprocess Data ------------------------\n",
    "df = pd.read_csv(\"final_dataset_air.csv\")\n",
    "df.dropna(subset=['NowCast Conc.', 'Raw Conc.', 'AQI'], inplace=True)\n",
    "\n",
    "X = df[['NowCast Conc.', 'Raw Conc.']].values\n",
    "y = df[['AQI']].values\n",
    "\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_x.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(SEQUENCE_LENGTH, len(X_scaled)):\n",
    "    X_seq.append(X_scaled[i-SEQUENCE_LENGTH:i])\n",
    "    y_seq.append(y_scaled[i])\n",
    "\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "split = int(0.8 * len(X_seq))\n",
    "X_train, X_test = X_seq[:split], X_seq[split:]\n",
    "y_train, y_test = y_seq[:split], y_seq[split:]\n",
    "\n",
    "# ------------------------ Define Model Factory ------------------------\n",
    "def build_model(model_name):\n",
    "    model = Sequential()\n",
    "    \n",
    "    if model_name == \"LSTM\":\n",
    "        model.add(LSTM(64, input_shape=(SEQUENCE_LENGTH, 2)))\n",
    "    elif model_name == \"GRU\":\n",
    "        model.add(GRU(64, input_shape=(SEQUENCE_LENGTH, 2)))\n",
    "    elif model_name == \"BiLSTM\":\n",
    "        model.add(Bidirectional(LSTM(64), input_shape=(SEQUENCE_LENGTH, 2)))\n",
    "    elif model_name == \"RNN\":\n",
    "        model.add(SimpleRNN(64, input_shape=(SEQUENCE_LENGTH, 2)))\n",
    "    elif model_name == \"CNN\":\n",
    "        model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(SEQUENCE_LENGTH, 2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "    elif model_name == \"CNN-LSTM\":\n",
    "        model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(SEQUENCE_LENGTH, 2)))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(LSTM(50))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name\")\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# ------------------------ Training Function ------------------------\n",
    "def train_and_save_model(model_name):\n",
    "    print(f\"\\n🔧 Training {model_name}...\")\n",
    "    model = build_model(model_name)\n",
    "\n",
    "    early_stop = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    os.makedirs(\"model\", exist_ok=True)\n",
    "    model.save(f\"model/{model_name.lower().replace('-', '_')}_model.h5\")\n",
    "    print(f\"✅ Saved model: model/{model_name.lower().replace('-', '_')}_model.h5\")\n",
    "\n",
    "# ------------------------ Save Scalers Once ------------------------\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "joblib.dump(scaler_x, \"model/scaler_x.save\")\n",
    "joblib.dump(scaler_y, \"model/scaler_y.save\")\n",
    "\n",
    "# ------------------------ Run Training ------------------------\n",
    "model_list = [\"LSTM\", \"GRU\", \"BiLSTM\", \"CNN\", \"CNN-LSTM\", \"RNN\"]\n",
    "\n",
    "if selected_model.lower() == \"all\":\n",
    "    for m in model_list:\n",
    "        train_and_save_model(m)\n",
    "else:\n",
    "    if selected_model not in model_list:\n",
    "        raise ValueError(f\"Invalid model: {selected_model}\")\n",
    "    train_and_save_model(selected_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
